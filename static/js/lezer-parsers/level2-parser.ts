// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
import {specializeKeyword, extendKeyword} from "./tokens"
export const parser = LRParser.deserialize({
  version: 14,
  states: "&`QYQPOOOOQO'#Cp'#CpQYQPOOOqQPO'#CgOOQO'#Cu'#CuOvQPO'#CjOOQO'#Cv'#CvO!OQPO'#ClOOQO'#Cw'#CwO!WQPO'#CmOOQO'#Cx'#CxO!`QPO'#CnOOQO'#Ck'#CkOOQO'#Cy'#CyO!hQPO'#CoOOQO'#Cf'#CfQlQPO'#CqQ!pQPOOOOQO-E6n-E6nOOQO'#Cr'#CrO#UQPO,59ROOQO'#Cs'#CsO#aQPO,59UOOQO-E6s-E6sOOQO-E6t-E6tOOQO,59W,59WOOQO-E6u-E6uOOQO,59X,59XOOQO-E6v-E6vOOQO,59Y,59YOOQO-E6w-E6wOOQO,59Z,59ZO#lQPO,59]OOQO-E6o-E6oOOQO-E6p-E6pO$WQPO1G.mOOQO'#Ct'#CtO$cQPO1G.oOOQO-E6q-E6qO$kQPO7+$ZOOQO-E6r-E6r",
  stateData: "$v~OWOSpOS~OQSORUOSWOTYOU]O[ROqPO~OVcO~OQSO[eO~ORUO[iO~OSWO[kO~OTYO[mO~OU]O[oO~OQSORUOSWOTYOU]O[RO~OPtOVcO[eO~O[eOn^aq^a~OqPOQeaReaSeaTeaUea[eanea~O[eOnZiqZi~OPtO[eO~O[eOn]qq]q~O",
  goto: "#unPPPPPPPPPPotPtttyyyt!O!Y!a!g!u!{#T#]#e#mV`OQaV_OQaV[OQaQQOSbQpRp`SaOQRqaQdRRrdQfTQsdUvfswRwuQudRxuUTOQaRgTUVOQaRhVUXOQaRjXUZOQaRlZU^OQaRn^",
  nodeNames: "âš  ask print forward turn color sleep is Comment Program Command Assign Text Ask Print Turtle Forward Turn Color Sleep",
  maxTerm: 33,
  nodeProps: [
    ["group", 15,"turtle"]
  ],
  skippedNodes: [0,8],
  repeatNodeCount: 10,
  tokenData: "#O~RXOYnYZ!]Zpnpq!bqsnst!gt;'Sn;'S;=`!V<%lOn~sU[~OYnZpnqsnt;'Sn;'S;=`!V<%lOn~!YP;=`<%ln~!bOq~~!gOp~~!lSW~OY!gZ;'S!g;'S;=`!x<%lO!g~!{P;=`<%l!g",
  tokenizers: [0],
  topRules: {"Program":[0,9]},
  specialized: [{term: 12, get: (value: any, stack: any) => (specializeKeyword(value, stack) << 1), external: specializeKeyword},{term: 12, get: (value: any, stack: any) => (extendKeyword(value, stack) << 1) | 1, external: extendKeyword, extend: true}],
  tokenPrec: 0
})
